{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f5099f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:05.779109Z",
     "iopub.status.busy": "2025-10-21T18:48:05.778620Z",
     "iopub.status.idle": "2025-10-21T18:48:05.784622Z",
     "shell.execute_reply": "2025-10-21T18:48:05.783596Z"
    },
    "papermill": {
     "duration": 0.01664,
     "end_time": "2025-10-21T18:48:05.786647",
     "exception": false,
     "start_time": "2025-10-21T18:48:05.770007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879efbd",
   "metadata": {
    "papermill": {
     "duration": 0.005006,
     "end_time": "2025-10-21T18:48:05.797502",
     "exception": false,
     "start_time": "2025-10-21T18:48:05.792496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee7a874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:05.811414Z",
     "iopub.status.busy": "2025-10-21T18:48:05.811004Z",
     "iopub.status.idle": "2025-10-21T18:48:05.817180Z",
     "shell.execute_reply": "2025-10-21T18:48:05.816087Z"
    },
    "papermill": {
     "duration": 0.014622,
     "end_time": "2025-10-21T18:48:05.819010",
     "exception": false,
     "start_time": "2025-10-21T18:48:05.804388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import f1_score\n",
    "# import numpy as np\n",
    "\n",
    "# # Load data\n",
    "# train_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')\n",
    "# test_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')\n",
    "\n",
    "# emotion_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "# # Feature extraction\n",
    "# vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "# X_train = vectorizer.fit_transform(train_df['text'])\n",
    "# X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# y_train = train_df[emotion_cols].values\n",
    "\n",
    "# # Train model\n",
    "# model = MultiOutputClassifier(LogisticRegression(max_iter=1000, C=1.0))\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Create submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'anger': predictions[:, 0],\n",
    "#     'fear': predictions[:, 1],\n",
    "#     'joy': predictions[:, 2],\n",
    "#     'sadness': predictions[:, 3],\n",
    "#     'surprise': predictions[:, 4]\n",
    "# })\n",
    "\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ce5ca",
   "metadata": {
    "papermill": {
     "duration": 0.00498,
     "end_time": "2025-10-21T18:48:05.830240",
     "exception": false,
     "start_time": "2025-10-21T18:48:05.825260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multi-Label Emotion Classification - Milestone 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedf631",
   "metadata": {
    "papermill": {
     "duration": 0.005186,
     "end_time": "2025-10-21T18:48:05.841715",
     "exception": false,
     "start_time": "2025-10-21T18:48:05.836529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ============================================================================\n",
    "## CELL 1: Import Libraries and Load Data\n",
    "### ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f02f33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:05.855485Z",
     "iopub.status.busy": "2025-10-21T18:48:05.855108Z",
     "iopub.status.idle": "2025-10-21T18:48:12.186452Z",
     "shell.execute_reply": "2025-10-21T18:48:12.185338Z"
    },
    "papermill": {
     "duration": 6.340374,
     "end_time": "2025-10-21T18:48:12.188327",
     "exception": false,
     "start_time": "2025-10-21T18:48:05.847953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully!\n",
      "Training samples: 6827\n",
      "Test samples: 1707\n",
      "\n",
      "First 3 rows:\n",
      "   id                                               text  anger  fear  joy  \\\n",
      "0   0  the dentist that did the work apparently did a...      1     0    0   \n",
      "1   1  i'm gonna absolutely ~~suck~~ be terrible duri...      0     1    0   \n",
      "2   2  bridge: so leave me drowning calling houston, ...      0     1    0   \n",
      "\n",
      "   sadness  surprise             emotions  \n",
      "0        1         0  ['anger' 'sadness']  \n",
      "1        1         0   ['fear' 'sadness']  \n",
      "2        1         0   ['fear' 'sadness']  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/2025-sep-dl-gen-ai-project/test.csv')\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bb446",
   "metadata": {
    "papermill": {
     "duration": 0.004858,
     "end_time": "2025-10-21T18:48:12.198711",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.193853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ============================================================================\n",
    "## CELL 2: Milestone Questions 1-7 - Label Analysis\n",
    "### ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d61ed51f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.212022Z",
     "iopub.status.busy": "2025-10-21T18:48:12.211641Z",
     "iopub.status.idle": "2025-10-21T18:48:12.279300Z",
     "shell.execute_reply": "2025-10-21T18:48:12.278274Z"
    },
    "papermill": {
     "duration": 0.077351,
     "end_time": "2025-10-21T18:48:12.281198",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.203847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MILESTONE QUESTIONS 1-7: LABEL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Q1: Most common emotion\n",
      "fear        3860\n",
      "sadness     2171\n",
      "surprise    1999\n",
      "joy         1660\n",
      "anger        808\n",
      "dtype: int64\n",
      "‚úÖ ANSWER: fear (3860 occurrences)\n",
      "\n",
      "üìä Q2: Instances with exactly 2 labels\n",
      "‚úÖ ANSWER: 2587\n",
      "\n",
      "Label distribution:\n",
      "label_count\n",
      "0     676\n",
      "1    2743\n",
      "2    2587\n",
      "3     706\n",
      "4     112\n",
      "5       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Q3: Joy and Sadness together\n",
      "‚úÖ ANSWER: 96\n",
      "\n",
      "üìä Q4: Percentage with Surprise\n",
      "‚úÖ ANSWER: 29.28%\n",
      "\n",
      "üìä Q5: Max difference in occurrence counts\n",
      "Max: fear (3860)\n",
      "Min: anger (808)\n",
      "‚úÖ ANSWER: 3052\n",
      "\n",
      "üìä Q6: Median word length\n",
      "‚úÖ ANSWER: 13.0\n",
      "\n",
      "üìä Q7: Correlation (anger & fear)\n",
      "‚úÖ ANSWER: 0.08\n",
      "\n",
      "Full correlation matrix:\n",
      "          anger  fear   joy  sadness  surprise\n",
      "anger      1.00  0.08 -0.19     0.09      0.02\n",
      "fear       0.08  1.00 -0.47     0.29      0.16\n",
      "joy       -0.19 -0.47  1.00    -0.32     -0.10\n",
      "sadness    0.09  0.29 -0.32     1.00     -0.12\n",
      "surprise   0.02  0.16 -0.10    -0.12      1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MILESTONE QUESTIONS 1-7: LABEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "emotion_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q1: Most common emotion\n",
    "# -------------------------------------------------------------------------\n",
    "emotion_counts = train_df[emotion_cols].sum()\n",
    "most_common_emotion = emotion_counts.idxmax()\n",
    "most_common_count = emotion_counts[most_common_emotion]\n",
    "least_common_emotion = emotion_counts.idxmin()\n",
    "\n",
    "print(f\"\\nüìä Q1: Most common emotion\")\n",
    "print(emotion_counts.sort_values(ascending=False))\n",
    "print(f\"‚úÖ ANSWER: {most_common_emotion} ({most_common_count} occurrences)\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q2: Instances with exactly 2 labels\n",
    "# -------------------------------------------------------------------------\n",
    "train_df['label_count'] = train_df[emotion_cols].sum(axis=1)\n",
    "exactly_2_labels = (train_df['label_count'] == 2).sum()\n",
    "\n",
    "print(f\"\\nüìä Q2: Instances with exactly 2 labels\")\n",
    "print(f\"‚úÖ ANSWER: {exactly_2_labels}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(train_df['label_count'].value_counts().sort_index())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q3: Joy and Sadness together\n",
    "# -------------------------------------------------------------------------\n",
    "joy_and_sadness = ((train_df['joy'] == 1) & (train_df['sadness'] == 1)).sum()\n",
    "print(f\"\\nüìä Q3: Joy and Sadness together\")\n",
    "print(f\"‚úÖ ANSWER: {joy_and_sadness}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q4: Percentage containing Surprise\n",
    "# -------------------------------------------------------------------------\n",
    "surprise_count = train_df['surprise'].sum()\n",
    "surprise_percentage = (surprise_count / len(train_df)) * 100\n",
    "print(f\"\\nüìä Q4: Percentage with Surprise\")\n",
    "print(f\"‚úÖ ANSWER: {surprise_percentage:.2f}%\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q5: Maximum difference\n",
    "# -------------------------------------------------------------------------\n",
    "max_diff = emotion_counts.max() - emotion_counts.min()\n",
    "print(f\"\\nüìä Q5: Max difference in occurrence counts\")\n",
    "print(f\"Max: {emotion_counts.idxmax()} ({emotion_counts.max()})\")\n",
    "print(f\"Min: {emotion_counts.idxmin()} ({emotion_counts.min()})\")\n",
    "print(f\"‚úÖ ANSWER: {max_diff}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q6: Median word length\n",
    "# -------------------------------------------------------------------------\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\n",
    "median_word_count = train_df['word_count'].median()\n",
    "print(f\"\\nüìä Q6: Median word length\")\n",
    "print(f\"‚úÖ ANSWER: {median_word_count}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q7: Correlation between anger and fear\n",
    "# -------------------------------------------------------------------------\n",
    "correlation = train_df['anger'].corr(train_df['fear'])\n",
    "print(f\"\\nüìä Q7: Correlation (anger & fear)\")\n",
    "print(f\"‚úÖ ANSWER: {correlation:.2f}\")\n",
    "\n",
    "# Full correlation matrix\n",
    "print(\"\\nFull correlation matrix:\")\n",
    "corr_matrix = train_df[emotion_cols].corr()\n",
    "print(corr_matrix.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86dca7",
   "metadata": {
    "papermill": {
     "duration": 0.005049,
     "end_time": "2025-10-21T18:48:12.291770",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.286721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Concept - Text Normalization\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d031c98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.305390Z",
     "iopub.status.busy": "2025-10-21T18:48:12.303519Z",
     "iopub.status.idle": "2025-10-21T18:48:12.313525Z",
     "shell.execute_reply": "2025-10-21T18:48:12.312256Z"
    },
    "papermill": {
     "duration": 0.018482,
     "end_time": "2025-10-21T18:48:12.315455",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.296973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCEPT 1: TEXT NORMALIZATION (Lowercasing)\n",
      "================================================================================\n",
      "\n",
      "Vocabulary size before: 7\n",
      "Vocabulary size after: 5\n",
      "Reduction: 2 unique words merged\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCEPT 1: TEXT NORMALIZATION (Lowercasing)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Why normalize?\n",
    "# \"Happy\", \"happy\", and \"HAPPY\" should be treated as the same word\n",
    "\n",
    "# Impact on vocabulary\n",
    "sample_texts = [\"Happy birthday!\", \"I'm happy today\", \"HAPPY times\"]\n",
    "words_before = set(\" \".join(sample_texts).split())\n",
    "words_after = set(\" \".join(sample_texts).lower().split())\n",
    "print(f\"\\nVocabulary size before: {len(words_before)}\")\n",
    "print(f\"Vocabulary size after: {len(words_after)}\")\n",
    "print(f\"Reduction: {len(words_before) - len(words_after)} unique words merged\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0438358",
   "metadata": {
    "papermill": {
     "duration": 0.005663,
     "end_time": "2025-10-21T18:48:12.326856",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.321193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Concept - Tokenization\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b56f1ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.340084Z",
     "iopub.status.busy": "2025-10-21T18:48:12.339643Z",
     "iopub.status.idle": "2025-10-21T18:48:12.383590Z",
     "shell.execute_reply": "2025-10-21T18:48:12.382525Z"
    },
    "papermill": {
     "duration": 0.0525,
     "end_time": "2025-10-21T18:48:12.385302",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.332802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCEPT 2: TOKENIZATION\n",
      "================================================================================\n",
      "Text: I'm loving this movie! It's amazing, isn't it?\n",
      "\n",
      "Simple split: [\"I'm\", 'loving', 'this', 'movie!', \"It's\", 'amazing,', \"isn't\", 'it?']\n",
      "Count: 8\n",
      "\n",
      "NLTK tokenize: ['I', \"'m\", 'loving', 'this', 'movie', '!', 'It', \"'s\", 'amazing', ',', 'is', \"n't\", 'it', '?']\n",
      "Count: 14\n",
      "\n",
      "Example from dataset:\n",
      "Text: the dentist that did the work apparently did a lousy job as in just a few years ...\n",
      "Tokens (first 10): ['the', 'dentist', 'that', 'did', 'the', 'work', 'apparently', 'did', 'a', 'lousy']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCEPT 2: TOKENIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tokenization = Breaking text into words/tokens\n",
    "# Why? ML models need individual words as features\n",
    "\n",
    "sample = \"I'm loving this movie! It's amazing, isn't it?\"\n",
    "print(f\"Text: {sample}\")\n",
    "\n",
    "# Method 1: Simple split (naive)\n",
    "tokens_simple = sample.split()\n",
    "print(f\"\\nSimple split: {tokens_simple}\")\n",
    "print(f\"Count: {len(tokens_simple)}\")\n",
    "\n",
    "# Method 2: NLTK (better - handles contractions, punctuation)\n",
    "tokens_nltk = word_tokenize(sample)\n",
    "print(f\"\\nNLTK tokenize: {tokens_nltk}\")\n",
    "print(f\"Count: {len(tokens_nltk)}\")\n",
    "\n",
    "# Key difference: NLTK separates \"I'm\" ‚Üí [\"I\", \"'m\"], \"isn't\" ‚Üí [\"is\", \"n't\"]\n",
    "# This is better for understanding linguistic structure\n",
    "\n",
    "# Apply to real data\n",
    "print(\"\\nExample from dataset:\")\n",
    "sample_row = train_df['text'].iloc[0]\n",
    "print(f\"Text: {sample_row[:80]}...\")\n",
    "print(f\"Tokens (first 10): {word_tokenize(sample_row.lower())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ef41d",
   "metadata": {
    "papermill": {
     "duration": 0.005318,
     "end_time": "2025-10-21T18:48:12.396323",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.391005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Concept - Punctuation Removal + Q8\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6dbab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.409679Z",
     "iopub.status.busy": "2025-10-21T18:48:12.409297Z",
     "iopub.status.idle": "2025-10-21T18:48:12.456097Z",
     "shell.execute_reply": "2025-10-21T18:48:12.454668Z"
    },
    "papermill": {
     "duration": 0.055948,
     "end_time": "2025-10-21T18:48:12.457929",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.401981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCEPT 3: PUNCTUATION REMOVAL\n",
      "================================================================================\n",
      "string.punctuation = !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "Original: Hello! How are you? I'm fine, thanks.\n",
      "Cleaned: Hello How are you Im fine thanks\n",
      "\n",
      "================================================================================\n",
      "Q8: CHARACTER REDUCTION AFTER REMOVING PUNCTUATION\n",
      "================================================================================\n",
      "Original character count: 549,100\n",
      "Cleaned character count: 531,406\n",
      "Characters removed: 17,694\n",
      "\n",
      "‚úÖ ANSWER Q8: 3.22% reduction\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCEPT 3: PUNCTUATION REMOVAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# What punctuation are we removing?\n",
    "print(f\"string.punctuation = {string.punctuation}\")\n",
    "\n",
    "# Example\n",
    "text_punct = \"Hello! How are you? I'm fine, thanks.\"\n",
    "print(f\"\\nOriginal: {text_punct}\")\n",
    "\n",
    "# Remove punctuation using translate (fastest method)\n",
    "text_no_punct = text_punct.translate(str.maketrans('', '', string.punctuation))\n",
    "print(f\"Cleaned: {text_no_punct}\")\n",
    "\n",
    "# Why remove?\n",
    "# 1. Reduces vocabulary: \"word\" vs \"word!\" vs \"word?\"\n",
    "# 2. Simplifies processing\n",
    "# 3. Most punctuation doesn't carry emotion info\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q8: Character reduction after removing punctuation\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Q8: CHARACTER REDUCTION AFTER REMOVING PUNCTUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count original characters\n",
    "original_text = ' '.join(train_df['text'].astype(str))\n",
    "original_chars = len(original_text)\n",
    "\n",
    "# Clean function: lowercase + remove punctuation\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "train_df['text_clean'] = train_df['text'].apply(clean_text)\n",
    "\n",
    "# Count cleaned characters\n",
    "cleaned_text = ' '.join(train_df['text_clean'])\n",
    "cleaned_chars = len(cleaned_text)\n",
    "\n",
    "# Calculate reduction\n",
    "reduction_pct = ((original_chars - cleaned_chars) / original_chars) * 100\n",
    "\n",
    "print(f\"Original character count: {original_chars:,}\")\n",
    "print(f\"Cleaned character count: {cleaned_chars:,}\")\n",
    "print(f\"Characters removed: {original_chars - cleaned_chars:,}\")\n",
    "print(f\"\\n‚úÖ ANSWER Q8: {reduction_pct:.2f}% reduction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8699ffb",
   "metadata": {
    "papermill": {
     "duration": 0.005375,
     "end_time": "2025-10-21T18:48:12.469179",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.463804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Concept - Stop Words\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6b74d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.482500Z",
     "iopub.status.busy": "2025-10-21T18:48:12.482136Z",
     "iopub.status.idle": "2025-10-21T18:48:12.520698Z",
     "shell.execute_reply": "2025-10-21T18:48:12.519197Z"
    },
    "papermill": {
     "duration": 0.047687,
     "end_time": "2025-10-21T18:48:12.522597",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.474910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCEPT 4: STOP WORDS\n",
      "================================================================================\n",
      "Total stop words in NLTK: 198\n",
      "\n",
      "First 30 stop words:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\"]\n",
      "\n",
      "Original: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "After removing stop words: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Removed: ['the', 'over', 'the']\n",
      "\n",
      "================================================================================\n",
      "Q9: PERCENTAGE OF UNIQUE WORDS THAT ARE STOP WORDS\n",
      "================================================================================\n",
      "Total words (with repetition): 105,749\n",
      "Unique words: 8,365\n",
      "Unique stop words found: 129\n",
      "\n",
      "‚úÖ ANSWER Q9: 1.54%\n",
      "\n",
      "Examples found: ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Concept - Stop Words + Q9\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCEPT 4: STOP WORDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Stop words = Common words with little semantic value\n",
    "# Examples: \"the\", \"is\", \"at\", \"which\", \"a\", \"an\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Total stop words in NLTK: {len(stop_words)}\")\n",
    "print(f\"\\nFirst 30 stop words:\\n{sorted(list(stop_words))[:30]}\")\n",
    "\n",
    "# Example: Effect of removing stop words\n",
    "sample = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = sample.lower().split()\n",
    "filtered = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "print(f\"\\nOriginal: {tokens}\")\n",
    "print(f\"After removing stop words: {filtered}\")\n",
    "print(f\"Removed: {[w for w in tokens if w in stop_words]}\")\n",
    "\n",
    "# Important note for emotion detection:\n",
    "# Be careful! \"not happy\" ‚Üí \"happy\" changes meaning completely!\n",
    "# For this milestone we remove them, but for modeling you might want to keep negations\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q9: Percentage of unique words that are stop words\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Q9: PERCENTAGE OF UNIQUE WORDS THAT ARE STOP WORDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all words from cleaned text\n",
    "all_words = cleaned_text.split()\n",
    "unique_words = {word for word in set(all_words) if word}  # Remove empty strings\n",
    "\n",
    "print(f\"Total words (with repetition): {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(unique_words):,}\")\n",
    "\n",
    "# Find stop words in our vocabulary\n",
    "unique_stopwords = unique_words.intersection(stop_words)\n",
    "print(f\"Unique stop words found: {len(unique_stopwords)}\")\n",
    "\n",
    "# Calculate percentage\n",
    "stopword_pct = (len(unique_stopwords) / len(unique_words)) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ ANSWER Q9: {stopword_pct:.2f}%\")\n",
    "print(f\"\\nExamples found: {sorted(list(unique_stopwords))[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3007e6",
   "metadata": {
    "papermill": {
     "duration": 0.005936,
     "end_time": "2025-10-21T18:48:12.534611",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.528675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Word Frequency Analysis + Q10\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21301a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.547785Z",
     "iopub.status.busy": "2025-10-21T18:48:12.547435Z",
     "iopub.status.idle": "2025-10-21T18:48:12.574384Z",
     "shell.execute_reply": "2025-10-21T18:48:12.573090Z"
    },
    "papermill": {
     "duration": 0.03566,
     "end_time": "2025-10-21T18:48:12.576242",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.540582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q10: 5TH MOST FREQUENT WORD (EXCLUDING STOP WORDS)\n",
      "================================================================================\n",
      "Total words after removing stop words: 52,236\n",
      "\n",
      "Top 20 most frequent words (excluding stop words):\n",
      "--------------------------------------------------\n",
      " 1. head                 :    539\n",
      " 2. eyes                 :    438\n",
      " 3. like                 :    394\n",
      " 4. back                 :    365\n",
      " 5. heart                :    334 ‚≠ê 5TH!\n",
      " 6. one                  :    323\n",
      " 7. face                 :    293\n",
      " 8. get                  :    291\n",
      " 9. time                 :    271\n",
      "10. still                :    271\n",
      "11. im                   :    253\n",
      "12. got                  :    243\n",
      "13. never                :    220\n",
      "14. hands                :    217\n",
      "15. really               :    206\n",
      "16. hand                 :    198\n",
      "17. felt                 :    196\n",
      "18. know                 :    194\n",
      "19. went                 :    189\n",
      "20. day                  :    188\n",
      "\n",
      "‚úÖ ANSWER Q10: 'heart' (appears 334 times)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Q10: 5TH MOST FREQUENT WORD (EXCLUDING STOP WORDS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter out stop words\n",
    "words_no_stops = [w for w in all_words if w not in stop_words and w != '']\n",
    "print(f\"Total words after removing stop words: {len(words_no_stops):,}\")\n",
    "\n",
    "# Count word frequencies using Counter\n",
    "word_freq = Counter(words_no_stops)\n",
    "top_20 = word_freq.most_common(20)\n",
    "\n",
    "print(\"\\nTop 20 most frequent words (excluding stop words):\")\n",
    "print(\"-\" * 50)\n",
    "for rank, (word, count) in enumerate(top_20, 1):\n",
    "    marker = \" ‚≠ê 5TH!\" if rank == 5 else \"\"\n",
    "    print(f\"{rank:2d}. {word:20s} : {count:6,}{marker}\")\n",
    "\n",
    "fifth_word = top_20[4][0]\n",
    "fifth_count = top_20[4][1]\n",
    "\n",
    "print(f\"\\n‚úÖ ANSWER Q10: '{fifth_word}' (appears {fifth_count:,} times)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a1e5b",
   "metadata": {
    "papermill": {
     "duration": 0.005554,
     "end_time": "2025-10-21T18:48:12.587915",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.582361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Concept - Lemmatization\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578e5b5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.601709Z",
     "iopub.status.busy": "2025-10-21T18:48:12.601329Z",
     "iopub.status.idle": "2025-10-21T18:48:12.611358Z",
     "shell.execute_reply": "2025-10-21T18:48:12.609934Z"
    },
    "papermill": {
     "duration": 0.019732,
     "end_time": "2025-10-21T18:48:12.613686",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.593954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCEPT 5: STEMMING\n",
      "================================================================================\n",
      "Stemming examples:\n",
      "  running         ‚Üí run\n",
      "  runs            ‚Üí run\n",
      "  ran             ‚Üí ran\n",
      "  runner          ‚Üí runner\n",
      "  easily          ‚Üí easili\n",
      "  fairly          ‚Üí fairli\n",
      "\n",
      "Emotion words:\n",
      "  loving          ‚Üí love\n",
      "  loved           ‚Üí love\n",
      "  loves           ‚Üí love\n",
      "  lovely          ‚Üí love\n",
      "  happier         ‚Üí happier\n",
      "  happiest        ‚Üí happiest\n",
      "  happiness       ‚Üí happi\n",
      "\n",
      "Original tokens: ['i', 'am', 'feeling', 'extremely', 'happy', 'and', 'loved', 'by', 'my', 'loving', 'family']\n",
      "Stemmed tokens:  ['i', 'am', 'feel', 'extrem', 'happi', 'and', 'love', 'by', 'my', 'love', 'famili']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Concept - Stemming\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCEPT 5: STEMMING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Stemming = Reducing words to their root form (stem)\n",
    "# Uses rules/heuristics (crude but fast)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Examples\n",
    "words_to_stem = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly']\n",
    "print(\"Stemming examples:\")\n",
    "for word in words_to_stem:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    print(f\"  {word:15s} ‚Üí {stemmed}\")\n",
    "\n",
    "# Emotions example\n",
    "emotion_words = ['loving', 'loved', 'loves', 'lovely', 'happier', 'happiest', 'happiness']\n",
    "print(\"\\nEmotion words:\")\n",
    "for word in emotion_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    print(f\"  {word:15s} ‚Üí {stemmed}\")\n",
    "\n",
    "# Apply to sample text\n",
    "sample_text = \"I am feeling extremely happy and loved by my loving family\"\n",
    "tokens = word_tokenize(sample_text.lower())\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(f\"\\nOriginal tokens: {tokens}\")\n",
    "print(f\"Stemmed tokens:  {stemmed_tokens}\")\n",
    "\n",
    "# Pros: Fast, reduces vocabulary\n",
    "# Cons: Can be too aggressive (\"university\" ‚Üí \"univers\"), loses meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f95a5788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:12.628730Z",
     "iopub.status.busy": "2025-10-21T18:48:12.628353Z",
     "iopub.status.idle": "2025-10-21T18:48:16.675335Z",
     "shell.execute_reply": "2025-10-21T18:48:16.674009Z"
    },
    "papermill": {
     "duration": 4.05691,
     "end_time": "2025-10-21T18:48:16.677138",
     "exception": false,
     "start_time": "2025-10-21T18:48:12.620228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE PREPROCESSING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Preprocessing examples:\n",
      "\n",
      "Original: I'm feeling extremely happy and loved!\n",
      "  Basic:    im feeling extremely happy and loved\n",
      "  Stemmed:  im feel extrem happi love\n",
      "  Lemma:    im feel extremely happy love\n",
      "\n",
      "Original: This is the worst movie I've ever seen.\n",
      "  Basic:    this is the worst movie ive ever seen\n",
      "  Stemmed:  worst movi ive ever seen\n",
      "  Lemma:    worst movie ive ever see\n",
      "\n",
      "Original: Surprisingly, the ending was better than expected.\n",
      "  Basic:    surprisingly the ending was better than expected\n",
      "  Stemmed:  surprisingli end better expect\n",
      "  Lemma:    surprisingly end better expect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Complete Preprocessing Pipeline\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def preprocess_text(text, method='lemma', remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        method: 'stem', 'lemma', or 'none'\n",
    "        remove_stopwords: Boolean to remove stop words\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stop words (optional)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # 5. Stemming or Lemmatization\n",
    "    if method == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    elif method == 'lemma':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
    "    \n",
    "    # 6. Join back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the pipeline\n",
    "test_texts = [\n",
    "    \"I'm feeling extremely happy and loved!\",\n",
    "    \"This is the worst movie I've ever seen.\",\n",
    "    \"Surprisingly, the ending was better than expected.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPreprocessing examples:\\n\")\n",
    "for text in test_texts:\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"  Basic:    {preprocess_text(text, method='none')}\")\n",
    "    print(f\"  Stemmed:  {preprocess_text(text, method='stem', remove_stopwords=True)}\")\n",
    "    print(f\"  Lemma:    {preprocess_text(text, method='lemma', remove_stopwords=True)}\")\n",
    "    print()\n",
    "\n",
    "# # Apply to dataset (choose your approach)\n",
    "# print(\"Applying to training data...\")\n",
    "# train_df['text_processed'] = train_df['text'].apply(\n",
    "#     lambda x: preprocess_text(x, method='lemma', remove_stopwords=False)\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Preprocessing complete!\")\n",
    "# print(f\"\\nExample comparison:\")\n",
    "# print(f\"Original: {train_df['text'].iloc[0][:100]}...\")\n",
    "# print(f\"Processed: {train_df['text_processed'].iloc[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d72357",
   "metadata": {
    "papermill": {
     "duration": 0.006403,
     "end_time": "2025-10-21T18:48:16.690362",
     "exception": false,
     "start_time": "2025-10-21T18:48:16.683959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02311950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:16.705136Z",
     "iopub.status.busy": "2025-10-21T18:48:16.703894Z",
     "iopub.status.idle": "2025-10-21T18:48:17.821672Z",
     "shell.execute_reply": "2025-10-21T18:48:17.820121Z"
    },
    "papermill": {
     "duration": 1.127362,
     "end_time": "2025-10-21T18:48:17.823906",
     "exception": false,
     "start_time": "2025-10-21T18:48:16.696544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['text_processed'] = train_df['text'].apply(\n",
    "    lambda x: preprocess_text(x, method='lemma', remove_stopwords=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642cc990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:17.838914Z",
     "iopub.status.busy": "2025-10-21T18:48:17.837941Z",
     "iopub.status.idle": "2025-10-21T18:48:17.848177Z",
     "shell.execute_reply": "2025-10-21T18:48:17.847115Z"
    },
    "papermill": {
     "duration": 0.019363,
     "end_time": "2025-10-21T18:48:17.849844",
     "exception": false,
     "start_time": "2025-10-21T18:48:17.830481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 10                               \" and burst into tears.\n",
      "11                         there's nothing but dead air.\n",
      "12     it's nice to make it from week to week without...\n",
      "13     the insides of my eyelids flashed bright color...\n",
      "14     well, now my mouth is just a big ball of ouchies.\n",
      "                             ...                        \n",
      "105         but oh god my nails are black call a priest.\n",
      "106                   so she didn't date my best friend?\n",
      "107                     a grin slowly formed on my face.\n",
      "108    he always assumed he was hallucinating or just...\n",
      "109    most mornings i drift in and out, slowly shaki...\n",
      "Name: text, Length: 100, dtype: object...\n",
      "Processed: 10                                   and burst into tear\n",
      "11                           theres nothing but dead air\n",
      "12     its nice to make it from week to week without ...\n",
      "13     the insides of my eyelids flash bright color a...\n",
      "14       well now my mouth be just a big ball of ouchies\n",
      "                             ...                        \n",
      "105            but oh god my nail be black call a priest\n",
      "106                     so she didnt date my best friend\n",
      "107                        a grin slowly form on my face\n",
      "108    he always assume he be hallucinate or just go ...\n",
      "109    most mornings i drift in and out slowly shake ...\n",
      "Name: text_processed, Length: 100, dtype: object...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {train_df['text'].iloc[10:][:100]}...\")\n",
    "print(f\"Processed: {train_df['text_processed'].iloc[10:][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d75fcfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:17.864451Z",
     "iopub.status.busy": "2025-10-21T18:48:17.864100Z",
     "iopub.status.idle": "2025-10-21T18:48:19.926552Z",
     "shell.execute_reply": "2025-10-21T18:48:19.925481Z"
    },
    "papermill": {
     "duration": 2.072017,
     "end_time": "2025-10-21T18:48:19.928731",
     "exception": false,
     "start_time": "2025-10-21T18:48:17.856714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['text_processed'] = train_df['text'].apply(\n",
    "    lambda x: preprocess_text(x, method='stem', remove_stopwords=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10e26d",
   "metadata": {
    "papermill": {
     "duration": 0.006057,
     "end_time": "2025-10-21T18:48:19.942719",
     "exception": false,
     "start_time": "2025-10-21T18:48:19.936662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ef9a099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:48:19.956556Z",
     "iopub.status.busy": "2025-10-21T18:48:19.956182Z",
     "iopub.status.idle": "2025-10-21T18:48:19.965519Z",
     "shell.execute_reply": "2025-10-21T18:48:19.964062Z"
    },
    "papermill": {
     "duration": 0.018522,
     "end_time": "2025-10-21T18:48:19.967353",
     "exception": false,
     "start_time": "2025-10-21T18:48:19.948831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 10                               \" and burst into tears.\n",
      "11                         there's nothing but dead air.\n",
      "12     it's nice to make it from week to week without...\n",
      "13     the insides of my eyelids flashed bright color...\n",
      "14     well, now my mouth is just a big ball of ouchies.\n",
      "                             ...                        \n",
      "105         but oh god my nails are black call a priest.\n",
      "106                   so she didn't date my best friend?\n",
      "107                     a grin slowly formed on my face.\n",
      "108    he always assumed he was hallucinating or just...\n",
      "109    most mornings i drift in and out, slowly shaki...\n",
      "Name: text, Length: 100, dtype: object...\n",
      "Processed: 10                                   and burst into tear\n",
      "11                               there noth but dead air\n",
      "12     it nice to make it from week to week without a...\n",
      "13     the insid of my eyelid flash bright color all ...\n",
      "14         well now my mouth is just a big ball of ouchi\n",
      "                             ...                        \n",
      "105           but oh god my nail are black call a priest\n",
      "106                     so she didnt date my best friend\n",
      "107                        a grin slowli form on my face\n",
      "108       he alway assum he wa hallucin or just go crazi\n",
      "109    most morn i drift in and out slowli shake off ...\n",
      "Name: text_processed, Length: 100, dtype: object...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {train_df['text'].iloc[10:][:100]}...\")\n",
    "print(f\"Processed: {train_df['text_processed'].iloc[10:][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390945d",
   "metadata": {
    "papermill": {
     "duration": 0.007105,
     "end_time": "2025-10-21T18:48:19.981328",
     "exception": false,
     "start_time": "2025-10-21T18:48:19.974223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13800781,
     "isSourceIdPinned": false,
     "sourceId": 115439,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.826247,
   "end_time": "2025-10-21T18:48:21.721531",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-21T18:47:58.895284",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
